# DEFAULT PREDICTION MLOPS PIPELINE
## _MLOps - проект по прогнозированию дефолта_

Прогнозирование дефолта - классическая задача, решаемая Data Scientist-ами повсеместно в финтех секторе.
Вероятность дефолта - одна из ключевых составляющих кредитного риска, в свою очередь влияющего на управление Банком своим портфелем и резервами.

Построение процесса скоринга и принятия на его основе решения представляет из себя интересную и многогранную задачу с возможностью использовать широкий спектр инструментов автоматизации.

Одним из вариантов построения архитектуры процесса является использование облачных технологий. Облачная инфраструктура для финтех решения должна покрываться более строгими контурами защиты информации, чем аналогичное on-premise решение, но вопросы безопасности выходят за рамки данной работы.

Далее пошагово описывается структура разработанного решения с методами использования различных технологий и применяемых инструментов.

## Данные

<image src="screens/1.jpg" alt="data">

Получение реальных данных о договорах и заемщиках вне банковских контуров - задача непосильная, а иногда и нелегльная :) Так, для разработки модели был выбран датасет деперсонализированных данных с сайта конкурсов по машинному обучению https://www.kaggle.com/competitions/loan-default-prediction/data

Это набор данных без указания наименования признаков, насчитывающий около 800 параметров и таргет 'loss' - указывающий одновременно и на факт дефолта, и на величину потерь в случае его реализации. Для задачи данного проекта таргет был преобразован в бинарную величину (1-был факт дефолта, 0-не было). Данные датасета соответствуют реальному набору финансовых операций, но анонимизированы, стандартизировны и лишены трендов.

В наборе данных присутствуют как дискретные, так и непрерывные и категориальные признаки. Объем датасета порядка 500 Мб.

## Поступление новых данных для инференса
В проекте выбран подход к искусственной генерации данных для инференса. От исходного train датасета была отделена часть на test, из которого, в последствии, сэмплируются "новые" данные, игнорируя наличие разметки.

Примечание: здесь можно интересным образом эмулировать процесс data leak и наблюдать устойчивый рост метрик - просто сэмплируя данные для инференса с возвращением, что позволит модели зафиксировать некоторые наблюдения дважды. Также случайное сэмплирование позволяет модели "заглядывать в будущее", если распределение данных схоже по поведению с временным рядом.

Процесс сэмплирования данных реализован в скрипте data_gen.py в папке data_gen

## Baseline python

Для построения baseline модели использовался случайно семплированная из исходного датасета выборка данных размером в 100 000 записей. В качестве метода машинного обучения выбрана библиотека от Яндекса - Catboost.

Пайплайн решения включает в себя небольшой EDA, предобработку признаков - заполнение пропусков в соответствии с типами, построение нисходящего списка важности переменных, а так же отбор наиболее информативных из них. После - оценку базового решения на кросс-валидации и forward feature selection на подмножесте отобранных признаков. Этап тюнинга гиперпараметров пропущен намеренно, в связи с тем, что классификатор хорошо себя показывает "из коробки" на дефолтных настройках.

Процесс моделирования можно увидеть в ноутбуке model.ipynb в папке model.

## Baseline PySpark
Особым челленджем проекта стало использование API для Spark - PySpark, в связи с желанием сделать решение масштабируемым на большие объемы данных, а вычисления - распределенными.
Разработка модели на PySpark представлена в файле spark_model.ipynb в папке spark

## Хранение данных
В батчовых моделях финтеха заказчик всегда знает, во сколько и где должен забирать выходы из продакшн-решений. Обычно это внутренние хранилища, но в рамках проекта для получения результатов инференса, логгирования артефактов и хранения наборов данных для обучения/дообучения используется s3 - объектное хранилище, разработанное компанией AWS и доступное в облачном сервисе Яндекса.

<image src="screens/3. s3 folders.jpg" alt="s3">

В хранилище представлены следующие папки:
1. train - исходный датасет для обучения модели
2. test - отложенная выборка для эмуляции потока новых данных
3. test_sample - сэплированная выборка из test размером 10%, она же "новые данные"
4. train_sup - обогащенная обучающая выборка для дообучения модели
5. test_sample_inference - выходные данные инференса - точка, где можно забирать скор для бизнеса
6. artifacts - папка хранения артефактов: гистограмм, графиков ROC_Curve, сериализованных "лучших" по результатам дообучения моделей, а так же различных иных метрик и величин.

<image src="screens/4. s3 pics.jpg" alt="s3">
<image src="screens/5. s3 models.jpg" alt="s3">




